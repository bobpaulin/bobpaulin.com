title=Tutorial - Hazelcast on AWS Fargate
date=2019-11-14
type=post
tags=tutorial, asciidoc
status=draft
~~~~~~
= Tutorial - Hazelcast on AWS Fargate
:imagesdir: /images
Bob Paulin
2019-11-14

=== Hazelcast

For a couple of years now when it comes to deciding on a all purpose cache for Java I've reached for Hazelcast.  Each time I've done it I've aways been impressed by the wide array of cluster options available.  In fact I've never used the same discovery/clustering configuration twice.  While exploring new ways of doing things is exciting I think the main driver of discovering new discovery mechninanisms is the changing needs of clients wanting to take advantage of cloud offerings that fit how they want to manage there stacks.

=== AWS ECS Fargate

Nearly all projects I've been on recently are working with containers and different ways of deploying them to the cloud.  It started with just putting docker on VMs but constant upgrading and devops overhead is leading many to start looking at more managed solutions.  While Kubernetes(K8) seems to be the most popular there are cases where K8 is overkill.  These projects involve maybe a dozen services maintained by a handfull of teams.  These teams have already gone through their battles with docker-compose and perhaps dabbled in swarm or rancher.  They look at services like Azure Kubernetes Service(AKS) or Amazon's Elastic Kubernetes Services and they see good and bad.  Good that they can have an entirely managed docker environment that avoids most of the pain of system patching and scaling.  But the bad is they have to learn an entirely new tool with it's own opinions, commands, and best practices.  Recently I've started exploring the middle ground between self hosted docker and Kubernetes and I've started to like AWS Elastic Container Service's (ECS) Fargate option.  AWS ECS has been around for some time but until Fargate came around you still saw all the EC2 instances spun up in the background so while a number of things were managed for you at the end of the day it's still docker on your VMs.   With Fargate the VMs no longer show up on your EC2 console.  They are completely managed by AWS.  

=== ECS-CLI

Another great benefit of ECS is it has a cli interface that have a number of useful features coming from a self hosted docker environment.  The first and by far most useful in my opinion is it's integration with docker compose.  Many docker projects I've been on have adopted compose soon after figuring out the docker basics to better manage all the parameters that go into spinning up a stack of docker containers.  Moving to Kubernetes requires these to be converted while ECS is able to use the docker-compose.yml nearly as is.  Notable exceptions are secrets which can be managed via the ecs-params.yml file by tieing them to SSM Parameter Store values.  The ecs-params.yml also allows you to set the memory and cpu limits that will be used to size your containiner in Fargate.  This is important since rather than paying directly for the underlieing EC2 instances with Fargate billing is based on cpu and memory set when you start your container.  The ecs-params.yml also allow you to set networking and security parameters within AWS to determine what service your container will have permission to interact with, what ports will be open, and which vpc subnet you'll be deployed into.  

=== DNS Based Discovery with AWS Cloud Map

Finally the CLI allows you to set up service discovery in AWS Cloud Map.  Cloud Map allows ECS to manage AWS Route53 so that services looking to call your container can use simple DNS A or SRV records.  This will be important as we try to configure Hazelcast since instances will need to be able to locate each other without having to redeploy configuration in the containers.  A records and SRV records do this in different ways.  When you think about a regular website like https://bobpaulin.com an A record is what connects bobpaulin.com to the ip address that serves the file content.  SRV records work a bit different but they allow more information to be passed to the caller such as the port to look on when looking up how to call the service.  When you start a service with ecs you can enable service discovery which add and removes DNS entries in Route53 as tasks are started and stopped within your cluster.

=== Hazelcast Kubernetes Plugin

The last piece of the puzzle to tie all these peices together is the Hazelcast Kubernetes plugin.  This plugin is included in the Hazelcast community docker image and contains two different method of doing discovery.  The first leverages the Kubernetes serivce API.  That api exists only in a K8 deployment so it won't be of much use in a non K8 environment.  The second is based on DNS.  This implementation relies on DNS SRV records so it could be leveraged in any environment that creates SRV records.  This is what we'll use in ECS since AWS Cloud Map can be setup to create service records.  This configuration will allow hazelcast instances to locate each other and form a cluster.

=== Setting up Hazelcast on Fargate

Prerequisites:

Docker installed
AWS Account
AWS CLI Installation
ECS CLI Installation
VPC with Internet Access

1) Create ECR for hazelcast 

[source,bash]
----
aws ecr create-repository --repository-name hazelcast-fargate
----

2) Create hazelcast.xml

Note the <interface> element entry should match the subnet that the Fargate task will be running in.  For example a CIDR of 172.30.4.0/24 would have: <interface>172.30.4.*</interface>

The service-dns value will be determined by the service discovery service name and the private dns namespace name defined later.

<service-discovery-name>.<private-dns-namespace-name>

[source,xml]
----
<?xml version="1.0" encoding="UTF-8"?>
<hazelcast xsi:schemaLocation="http://www.hazelcast.com/schema/config hazelcast-config-3.7.xsd"
           xmlns="http://www.hazelcast.com/schema/config"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">

    <properties>
        <property name="hazelcast.discovery.enabled">true</property>
    </properties>
    
    <management-center enabled="false" update-interval="2">http://localhost:8080/mancenter</management-center>
       
   <network>
        <join>
          <!-- deactivate multicast which is enabled by default -->
          <multicast enabled="false"/>
          <aws enabled="false"/>
          <tcp-ip enabled="false" />
          <discovery-strategies>
            <discovery-strategy enabled="true"
                class="com.hazelcast.kubernetes.HazelcastKubernetesDiscoveryStrategy">
              <properties>
                 <property name="service-dns">hazelcast-poc.hazelcast.bobpaulin.com</property>
              </properties>
            </discovery-strategy>
          </discovery-strategies>
        </join>
        <interfaces enabled="true">
          <interface>172.30.4.*</interface>
        </interfaces>
    </network>
</hazelcast>
----

3) Create Dockerfile

[source,Dockerfile]
----

FROM hazelcast/hazelcast:3.11.4

ADD hazelcast.xml $HZ_HOME

----

4) Build docker image

Build image with custom hazelcast.xml
The tag for the ECR should come from the repositoryUri output from the command

[source,bash]
----
aws ecr describe-repositories
----

Next build and tag the image

[source,bash]
----
docker build -t hazelcast-fargate .
docker tag hazelcast-fargate 11111111111.dkr.ecr.us-east-1.amazonaws.com/hazelcast-fargate:3.11.4
----

5) Deploy the docker image to ECR

Login to ECR 

[source,bash]
----
$(aws ecr get-login --no-include-email)
----


Push Container

[source,bash]
----
docker push 11111111111.dkr.ecr.us-east-1.amazonaws.com/hazelcast-fargate:3.11.4
----


6) Create Cloudwatch Log Group

[source,bash]
----
aws logs create-log-group --log-group-name /ecs/bobpaulin/hazelcast-aws
----

7) Create Task Execution Role

task-execution-assume-role.json
[source,json]
----
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "",
      "Effect": "Allow",
      "Principal": {
        "Service": "ecs-tasks.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
----

Run the following aws cli command to create the role

[source,bash]
----
aws iam --region us-east-1 create-role --role-name ecsTaskExecutionRole --assume-role-policy-document file://task-execution-assume-role.json
----

Run the following aws cli command to attach the role policy

[source,json]
----
aws iam --region us-east-1 attach-role-policy --role-name ecsTaskExecutionRole --policy-arn arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy
----

8) Configuring the Security Group

[source,bash]
----
aws ec2 create-security-group --group-name EcsHazelcastSecurityGroup --description "Hazelcast ECS Security Group" --vpc vpc-abcdefg
----

Add ingress port rules

[source,bash]
----
aws ec2 authorize-security-group-ingress --group-id sg-123456789 --protocol tcp --port 5701 --cidr 0.0.0.0/0
----

9) Creating a docker-compose.yml

[source,yaml]
----
version: '3'
services:
  hazelcast-service:
    image: 11111111111.dkr.ecr.us-east-1.amazonaws.com/hazelcast-fargate:3.11.4
    ports:
      - "5701:5701" 
    logging:
      driver: awslogs
      options: 
        awslogs-group: /ecs/bobpaulin/hazelcast-aws 
        awslogs-region: us-east-1
        awslogs-stream-prefix: ecs
    environment:
      - MIN_HEAP_SIZE=4g
      - MAX_HEAP_SIZE=4g 
      - AWS_DEFAULT_REGION=us-east-1
----

10) Creating a ecs-params.yml

[source,yaml]
----
version: 1
task_definition:
  task_execution_role: ecsTaskExecutionRole
  ecs_network_mode: awsvpc 
  task_size:
    mem_limit: 6.0GB
    cpu_limit: 2048 
run_params:
  network_configuration:
    awsvpc_configuration:
      subnets:
        - "subnet-abcdefg"
      security_groups:
        - "sg-123456789"
  service_discovery:
    private_dns_namespace:
      vpc: "vpc-098765"
      name: "hazelcast.bobpaulin.com"
    service_discovery_service:
      name: "hazelcast-poc"
      dns-config:
        type: SRV
        ttl: 120
----

11) Configuring the ecs-cli to point to he cluster

[source,bash]
----
ecs-cli configure --cluster hazelcast --default-launch-type FARGATE --config-name default --region us-east-1
----

Configure Profile

Replace <AWS_ACCESS_KEY_ID> and <AWS_SECRET_ACCESS_KEY> with your AWS Access Key and Access Secret respectively.

[source,bash]
----
ecs-cli configure profile --access-key <AWS_ACCESS_KEY_ID> --secret-key <AWS_SECRET_ACCESS_KEY> --profile-name default-profile
----

12) Running the ecs-cli to create the cluster

[source,bash]
----
ecs-cli up --cluster-config default --ecs-profile default-profile --security-group sg-123456789 --vpc vpc-098765 --subnets subnet-abcdefg
----

Create ecs

[source,bash]
----
ecs-cli compose --project-name hazelcast-service service up --cluster hazelcast --enable-service-discovery --dns-type SRV --sd-container-name hazelcast-service --sd-container-port 5701 
----

Scale it up!

[source,bash]
----
ecs-cli compose --project-name hazelcast-service service scale 3
----

Verify the cluster is formed from the logs

[source,text]
----
2019-11-18 22:35:34
INFO: [172.30.4.67]:5701 [dev] [3.11.4]
2019-11-18 22:35:34
Members {size:3, ver:3} [
2019-11-18 22:35:34
Member [172.30.4.67]:5701 - f8044a27-e20e-45bd-adba-fcac4e069cc1 this
2019-11-18 22:35:34
Member [172.30.4.241]:5701 - a69055e8-40d7-4cad-b5c1-8dcfd008f766
2019-11-18 22:35:34
Member [172.30.4.236]:5701 - 04ad412e-bc5b-4673-9226-12f8c60a1f06
2019-11-18 22:35:34
]
----

13) Turn it off!


Remove the Service

[source,bash]
----
ecs-cli compose --project-name hazelcast-service service rm --cluster hazelcast 
----

Remove the Cluster

[source,bash]
----
ecs-cli down --cluster-config default --ecs-profile default-profile
----